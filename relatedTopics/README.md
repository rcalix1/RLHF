## Related Topics

* https://medium.com/deeplearningmadeeasy/simple-ppo-implementation-e398ca0f2e7c
* https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-1-4-613dfc1b14c8
* https://lightning.ai/lightning-ai/studios/code-lora-from-scratch?view=public&section=all
* https://medium.com/polo-club-of-data-science/multi-gpu-training-in-pytorch-with-code-part-4-torchrun-93c44c55e8eb
* https://towardsdatascience.com/distribute-your-pytorch-model-in-less-than-20-lines-of-code-61a786e6e7b0
* https://jacksoncakes.com/2023/08/20/getting-started-with-distributed-data-parallel-in-pytorch-a-beginners-guide/
* https://dongdongbh.tech/ddp/
* https://ochzhen.com/blog/pytorch-distributed-data-parallel-azure-ml
* https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html
* https://tuni-itc.github.io/wiki/Technical-Notes/Distributed_dataparallel_pytorch/
* https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md
* https://opacus.ai/tutorials/ddp_tutorial
* https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html
* https://discuss.pytorch.org/t/use-distributed-data-parallel-correctly/82500
* https://medium.com/@ManishChablani/aligning-llms-with-direct-preference-optimization-dpo-background-overview-intuition-and-paper-0a72b9dc539c
* https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb
* 

## How to run a 100% local, fully private LLM with llama.cpp ðŸ”¥

* 2 lines of code, OpenAI compatible!
* Step 1: brew install llama.cpp
* Step 2: llama-server --hf-repo microsoft/Phi-3-mini-4k-instruct-gguf --hf-file Phi-3-mini-4k-instruct-q4.gguf
* Step 3: curl 8080/v1/chat/completions
* P.S. You can point to any GGUF on the HF hub ðŸ”¥
* https://github.com/aleksanderhan/turbo-genius
* https://github.com/context-labs/mactop

## Local LLAMA

* https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/03_model-evaluation/llm-instruction-eval-ollama.ipynb
