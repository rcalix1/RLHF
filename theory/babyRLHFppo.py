# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G96wRwADuv32h6uVOHTTibM-TU_eRLlz
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# === Tiny GPT-style Policy ===
class TinyPolicy(nn.Module):
    def __init__(self, vocab_size=10, hidden_dim=16):

        super().__init__()
        self.embed = nn.Embedding(vocab_size, hidden_dim)
        self.lm_head = nn.Linear(hidden_dim, vocab_size)  # Predict next-token logits

    def forward(self, input_ids):
        x = self.embed(input_ids)          # [seq_len, hidden]
        last_hidden = x[-1]                # Use last token's embedding
        logits = self.lm_head(last_hidden) # [vocab_size]
        return logits  # logits over vocab for next token

# === Reward function: +1 if token == 1 ===
def fake_reward(token_ids):
    return torch.where(token_ids == 1, torch.tensor(1.0), torch.tensor(0.0))

# === PPO Loss Function ===
def ppo_loss(new_logprobs, old_logprobs, rewards, kl_coeff=0.1):

    print( new_logprobs )
    print( old_logprobs )
    print( rewards.shape )

    ratio = torch.exp(new_logprobs - old_logprobs)
    print( ratio )
    input()

    surrogate = ratio * rewards
    kl = (old_logprobs - new_logprobs).mean()
    return -surrogate.mean() + kl_coeff * kl

# === Training Loop ===
vocab_size = 10
policy = TinyPolicy(vocab_size)
optimizer = optim.Adam(policy.parameters(), lr=1e-2)



prompt = torch.tensor([0])  # Start with a fixed token (e.g., BOS token 0)
generated = [0]

old_logprobs = []
rewards = []

prompt.shape



embed = nn.Embedding(vocab_size, 16)

lm_head = nn.Linear(16, vocab_size)  # Predict next-token logits

input_ids = prompt
x = embed(input_ids)          # [seq_len, hidden]

print(x.shape)
print(x)

last_hidden = x[-1]                # Use last token's embedding

print( last_hidden.shape)
print( last_hidden)

logits = lm_head(last_hidden) # [vocab_size]
print( logits.shape )
print( logits )

probs = F.softmax(logits, dim=-1)
print( probs.shape )
print( probs )

# torch.multinomial adds controlled randomness — usually picks high-prob tokens like index 8
# but sometimes explores others — perfect for RLHF-style exploration vs argmax



next_token = torch.multinomial(probs, num_samples=1)  # Sample next token
next_token

probs[next_token]

logprob = torch.log(probs[next_token])
logprob

old_logprobs.append(logprob)

next_token

fake_reward(next_token)

next_token

token_ids = torch.tensor(9)
token_ids

torch.where(token_ids == 1, torch.tensor(1.0), torch.tensor(0.0))

rewards.append( fake_reward(next_token)   )
generated.append(next_token.item())

prompt

next_token

prompt = torch.cat([prompt, next_token], dim=0)

prompt

generated

generated_tensor = torch.tensor(generated[1:])  # drop BOS token
generated_tensor

rewards

rewards = torch.stack(rewards).squeeze()
rewards

old_logprobs

probs = torch.tensor([0.2, 0.7, 0.1])
logprobs2 = torch.log(probs)

logprobs2

old_logprobs = torch.stack(old_logprobs).squeeze()
old_logprobs.shape

old_logprobs

new_logprobs = []
prompt = torch.tensor([0])

prompt





for step in range(300):
    prompt = torch.tensor([0])  # Start with a fixed token (e.g., BOS token 0)
    generated = [0]

    old_logprobs = []
    rewards = []

    for _ in range(8):  # Generate 8 tokens step-by-step
        logits = policy(prompt)
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)  # Sample next token
        logprob = torch.log(probs[next_token])

        # Save data
        old_logprobs.append(logprob)
        rewards.append(fake_reward(next_token))
        generated.append(next_token.item())

        # Update prompt
        prompt = torch.cat([prompt, next_token], dim=0)

    generated_tensor = torch.tensor(generated[1:])  # drop BOS token
    rewards = torch.stack(rewards).squeeze()
    old_logprobs = torch.stack(old_logprobs).squeeze()

    # === Compute new logprobs under current policy ===
    new_logprobs = []
    prompt = torch.tensor([0])
    for token in generated_tensor:
        logits = policy(prompt)
        probs = F.softmax(logits, dim=-1)
        logprob = torch.log(probs[token])
        new_logprobs.append(logprob)
        prompt = torch.cat([prompt, token.unsqueeze(0)], dim=0)

    new_logprobs = torch.stack(new_logprobs)

    # === Compute PPO loss ===
    loss = ppo_loss(new_logprobs, old_logprobs, rewards)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 50 == 0:
        avg_reward = rewards.float().mean().item()
        print(f"Step {step} | Loss: {loss.item():.4f} | Avg Reward: {avg_reward:.2f}")













